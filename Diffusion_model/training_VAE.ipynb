{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "from tqdm import trange\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "import scipy.constants as const  # Delete them when updating the data \n",
    "from scipy.integrate import quad # Delete them when updating the data \n",
    "import joblib\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(0)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Properties = pd.read_csv('files/Data/Properties.csv')\n",
    "Compositions = pd.read_csv('files/Data/Compositions.csv')\n",
    "\n",
    "'''\n",
    "Properties indices:\n",
    " 0: 'Density'\n",
    " 1: 'Young's modulus'\n",
    " 2: 'Flexural modulus'\n",
    " 3: 'Shear modulus'\n",
    " 4: 'Bulk modulus'\n",
    " 5: 'Poisson's ratio'\n",
    " 6: 'Melting point'\n",
    " 7: 'Thermal conductivity'\n",
    " 8: 'Specific heat capacity'\n",
    " 9: 'Thermal expansion coefficient'\n",
    "10: 'Latent heat of fusion'\n",
    "11: 'Electrical conductivity'\n",
    "12: 'Acoustic velocity'\n",
    "13: 'Average Atomic Weight'\n",
    "'''\n",
    "\n",
    "'''\n",
    "Compositions indices:\n",
    "  0: 'Ag (silver)'\n",
    "  1: 'Al (aluminum)'\n",
    "  2: 'As (arsenic)'\n",
    "  3: 'Au (gold)'\n",
    "  4: 'B (boron)'\n",
    "  5: 'Be (beryllium)'\n",
    "  6: 'BeO (beryllia)'\n",
    "  7: 'Bi (bismuth)'\n",
    "  8: 'C (carbon)'\n",
    "  9: 'Ca (calcium)'\n",
    " 10: 'Cd (cadmium)'\n",
    " 11: 'Ce (cerium)'\n",
    " 12: 'Co (cobalt)'\n",
    " 13: 'Cr (chromium)'\n",
    " 14: 'Cu (copper)'\n",
    " 15: 'Dy (dysprosium)'\n",
    " 16: 'Er (erbium)'\n",
    " 17: 'Eu (europium)'\n",
    " 18: 'Fe (iron)'\n",
    " 19: 'Ga (gallium)'\n",
    " 20: 'Gd (gadolinium)'\n",
    " 21: 'Ge (germanium)'\n",
    " 22: 'H (hydrogen)'\n",
    " 23: 'Hf (hafnium)'\n",
    " 24: 'Ho (holmium)'\n",
    " 25: 'In (indium)'\n",
    " 26: 'Ir (iridium)'\n",
    " 27: 'La (lanthanum)'\n",
    " 28: 'Li (lithium)'\n",
    " 29: 'Lu (lutetium)'\n",
    " 30: 'Mg (magnesium)'\n",
    " 31: 'Mn (manganese)'\n",
    " 32: 'Mo (molybdenum)'\n",
    " 33: 'N (nitrogen)'\n",
    " 34: 'Nb (niobium)'\n",
    " 35: 'Nd (neodymium)'\n",
    " 36: 'Ni (nickel)'\n",
    " 37: 'O (oxygen)'\n",
    " 38: 'O2 (oxygen gas)'\n",
    " 39: 'Os (osmium)'\n",
    " 40: 'P (phosphorus)'\n",
    " 41: 'Pb (lead)'\n",
    " 42: 'Pd (palladium)'\n",
    " 43: 'Pr (praseodymium)'\n",
    " 44: 'Pt (platinum)'\n",
    " 45: 'Re (rhenium)'\n",
    " 46: 'Rh (rhodium)'\n",
    " 47: 'Ru (ruthenium)'\n",
    " 48: 'S (sulfur)'\n",
    " 49: 'Sb (antimony)'\n",
    " 50: 'Sc (scandium)'\n",
    " 51: 'Se (selenium)'\n",
    " 52: 'Si (silicon)'\n",
    " 53: 'Sm (samarium)'\n",
    " 54: 'Sn (tin)'\n",
    " 55: 'Sr (strontium)'\n",
    " 56: 'Ta (tantalum)'\n",
    " 57: 'Tb (terbium)'\n",
    " 58: 'Te (tellurium)'\n",
    " 59: 'ThO2 (thoria)'\n",
    " 60: 'Ti (titanium)'\n",
    " 61: 'Tl (thallium)'\n",
    " 62: 'Tm (thulium)'\n",
    " 63: 'U (uranium)'\n",
    " 64: 'V (vanadium)'\n",
    " 65: 'Y (Yttrium)'\n",
    " 66: 'Yb (Ytterbium)'\n",
    " 67: 'W (Tungsten)'\n",
    " 68: 'Zn (Zinc)'\n",
    " 69: 'Zr (Zirconium)'\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = joblib.load('files/Scalers/scaler_Properties.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Compositions\n",
    "Y = Properties\n",
    "\n",
    "XX = X.values\n",
    "YY = scaler_y.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 15\n",
    "\n",
    "z_dim = latent_dim\n",
    "\n",
    "c_hidden_dim = [latent_dim, 128, 128, 128]\n",
    "\n",
    "epochs = 200\n",
    "dropout = 0.\n",
    "valid_size = 0.05\n",
    "batch_size = 32\n",
    "test_batch_size = batch_size\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "ResultsFolder = 'files/results'\n",
    "savedModelFolder = 'files/Models'\n",
    "load_pretrained_model = False\n",
    "\n",
    "c_lr = 1e-3\n",
    "learningRate = 5e-4\n",
    "\n",
    "param_Dim = YY.shape[1]\n",
    "add_noise = False\n",
    "recon_criterion = nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "Comp_vec_dim = XX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetching wrapper for efficient data loading\n",
    "class data_prefetcher():\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_data = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            return\n",
    "           \n",
    "    def next(self):\n",
    "        data = self.next_data\n",
    "        self.preload()\n",
    "        return data\n",
    "\n",
    "# Background prefetch-enabled DataLoader\n",
    "class DataLoaderX(DataLoader):\n",
    "    def __iter__(self):\n",
    "        return BackgroundGenerator(super().__iter__())\n",
    "\n",
    "\n",
    "# Custom Variational Autoencoder Model for property vector encoding and reconstruction\n",
    "class vaeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vaeModel, self).__init__()\n",
    "\n",
    "        # ---- Encoder Network ----\n",
    "        # Maps input vectors to latent distribution (mu and log_var)\n",
    "        x_hidden_dim = [Comp_vec_dim, 128, 128, 128, latent_dim]\n",
    "\n",
    "        # Feed-forward layers for encoding\n",
    "        self.en_x_fc1 = nn.Linear(x_hidden_dim[0], x_hidden_dim[1])\n",
    "        self.en_x_fc2 = nn.Linear(x_hidden_dim[1], x_hidden_dim[2])\n",
    "        self.en_x_fc3 = nn.Linear(x_hidden_dim[2], x_hidden_dim[3])\n",
    "        self.en_x_fc4 = nn.Linear(x_hidden_dim[3], x_hidden_dim[4])\n",
    "\n",
    "        # Latent space parameters (mean and log variance)\n",
    "        self.x_fc_mu = nn.Linear(latent_dim,  latent_dim)\n",
    "        self.x_fc_std = nn.Linear(latent_dim,  latent_dim)\n",
    "\n",
    "        # ---- Decoder Network ----\n",
    "        # Maps latent vector z back to the original input space\n",
    "        de_x_hidden_dim = [latent_dim, 128, 128, 128, Comp_vec_dim]\n",
    "\n",
    "        # Feed-forward layers for decoding\n",
    "        self.de_x_fc1 = nn.Linear(de_x_hidden_dim[0], de_x_hidden_dim[1])\n",
    "        self.de_x_fc2 = nn.Linear(de_x_hidden_dim[1], de_x_hidden_dim[2])\n",
    "        self.de_x_fc3 = nn.Linear(de_x_hidden_dim[2], de_x_hidden_dim[3])\n",
    "        self.de_x_fc4 = nn.Linear(de_x_hidden_dim[3], de_x_hidden_dim[4])\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample z from N(mu, sigma^2) using\n",
    "        a standard normal distribution.\n",
    "        \"\"\"\n",
    "        sigma = torch.exp(logvar)\n",
    "        eps = torch.randn_like(sigma)  # Sample epsilon from N(0, I)\n",
    "        return mu + sigma * eps\n",
    "\n",
    "    def encoder(self, x):\n",
    "        \"\"\"\n",
    "        Encodes input vector into latent distribution parameters.\n",
    "        \"\"\"\n",
    "        x = self.activation(self.en_x_fc1(x))\n",
    "        x = self.activation(self.en_x_fc2(x))\n",
    "        x = self.activation(self.en_x_fc3(x))\n",
    "        x = self.en_x_fc4(x)\n",
    "\n",
    "        x_mu = self.x_fc_mu(x)\n",
    "        x_log_var = self.x_fc_std(x)\n",
    "\n",
    "        x_z = self.reparameterize(x_mu, x_log_var)\n",
    "        return x_z, x_mu, x_log_var\n",
    "\n",
    "    def decoder(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the latent vector z back to the input space.\n",
    "        \"\"\"\n",
    "        x = self.activation(self.de_x_fc1(z))\n",
    "        x = self.activation(self.de_x_fc2(x))\n",
    "        x = self.activation(self.de_x_fc3(x))\n",
    "        x = self.de_x_fc4(x)\n",
    "\n",
    "        # Apply softmax to produce normalized probability-like output\n",
    "        return torch.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class pModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pModel, self).__init__()\n",
    "        self.dropout_p = 0.0\n",
    "\n",
    "        p_hidden_dim = [latent_dim, 128, 128, 128]  # Easy to modify\n",
    "\n",
    "        # Define the layers explicitly\n",
    "        self.e_fc1 = nn.Linear(latent_dim, p_hidden_dim[0], bias=False)\n",
    "        self.e_relu1 = nn.ReLU()\n",
    "\n",
    "        self.e_fc2 = nn.Linear(p_hidden_dim[0], p_hidden_dim[1], bias=False)\n",
    "        self.e_relu2 = nn.ReLU()\n",
    "        self.e_dropout2 = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.e_fc3 = nn.Linear(p_hidden_dim[1], p_hidden_dim[2], bias=False)\n",
    "        self.e_relu3 = nn.ReLU()\n",
    "        self.e_dropout3 = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.e_fc4 = nn.Linear(p_hidden_dim[2], p_hidden_dim[3], bias=False)\n",
    "        self.e_relu4 = nn.ReLU()\n",
    "        self.e_dropout4 = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.de_out = nn.Linear(p_hidden_dim[3], param_Dim, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.e_relu1(self.e_fc1(z))\n",
    "        x = self.e_relu2(self.e_fc2(x))\n",
    "        x = self.e_dropout2(x)\n",
    "        x = self.e_relu3(self.e_fc3(x))\n",
    "        x = self.e_dropout3(x)\n",
    "        x = self.e_relu4(self.e_fc4(x))\n",
    "        x = self.e_dropout4(x)\n",
    "        x = self.de_out(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def kld_loss(mu, logvar):\n",
    "    \"\"\"\n",
    "    Compute the Kullback-Leibler Divergence (KLD) loss between a Gaussian\n",
    "    posterior with mean `mu` and log-variance `logvar`, and the standard normal prior.\n",
    "    \"\"\"\n",
    "    # Apply the closed-form expression for KLD between two Gaussians\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return KLD\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Initialize weights of layers using Xavier uniform initialization.\n",
    "    Bias terms (if present) are initialized to zero.\n",
    "    \"\"\"\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        # Initialize weights with Xavier uniform distribution\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        # Zero-initialize biases if they exist\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(input, target, weight):\n",
    "    return (weight * (input - target) ** 2).sum()\n",
    "\n",
    "\n",
    "def properties_weighted_loss(p_pred, p):\n",
    "    \"\"\"\n",
    "    Computes the sum of weighted MSE losses over all stiffness parameters.\n",
    "    Currently, all weights are set to 1, but the structure allows future customization.\n",
    "    \"\"\"\n",
    "    p_mse = 0.0\n",
    "    weight = 1.0  # Uniform weight; can be customized per index below if needed\n",
    "\n",
    "    for i in range(param_Dim):\n",
    "        # Apply the same weight to all parameters for now.\n",
    "        # Keep this structure if you plan to assign different weights later.\n",
    "        p_mse += weighted_mse_loss(p_pred[:, i], p[:, i], weight)\n",
    "\n",
    "    return p_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_weight = 1\n",
    "p_weight = np.ones([epochs])*100 # constant weight of stiffness prediction loss\n",
    "model = vaeModel()\n",
    "p_model = pModel()\n",
    "model.to(device)\n",
    "p_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, p_model, test_loader, saveResults, test_batch_size):\n",
    "    # Set models to evaluation mode\n",
    "    model.eval()\n",
    "    p_model.eval()\n",
    "\n",
    "    # Initialize test losses\n",
    "    x_test_mse     = 0.0\n",
    "    p_test_mse     = 0.0\n",
    "    test_kld_loss  = 0.0\n",
    "\n",
    "    # Containers for predictions and ground truths\n",
    "    x_test = []\n",
    "    x_pred = []\n",
    "    p_test = []\n",
    "    p_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            X = data[0].to(device)\n",
    "            Y = data[1].to(device)\n",
    "\n",
    "            # Encode input\n",
    "            encoded, mu, std = model.encoder(X)\n",
    "\n",
    "            # Use either encoded vector or mean vector as input\n",
    "            p_input = encoded if add_noise else mu\n",
    "\n",
    "            # Predict properties and reconstruct input\n",
    "            p_pred_batch = p_model(p_input)\n",
    "            x_decoded    = model.decoder(encoded)\n",
    "\n",
    "            # Compute reconstruction and KLD losses\n",
    "            x_mse        = recon_criterion(x_decoded, X)\n",
    "            test_kld     = kld_loss(mu, std)\n",
    "            p_mse        = properties_weighted_loss(p_pred_batch, Y)\n",
    "\n",
    "            # Accumulate losses\n",
    "            x_test_mse  += x_mse.item()\n",
    "            test_kld_loss += test_kld.item()\n",
    "            p_test_mse  += p_mse.item()\n",
    "\n",
    "            # Store batch results\n",
    "            x_test.append(X.cpu().numpy())\n",
    "            x_pred.append(x_decoded.cpu().numpy())\n",
    "            p_test.append(Y.cpu().numpy())\n",
    "            p_pred.append(p_pred_batch.cpu().numpy())\n",
    "\n",
    "    # Merge all batches\n",
    "    x_test = np.concatenate(x_test)\n",
    "    x_pred = np.concatenate(x_pred)\n",
    "    p_test = np.concatenate(p_test)\n",
    "    p_pred = np.concatenate(p_pred)\n",
    "\n",
    "    # Apply inverse transform on predicted and true property values\n",
    "    x_test1 = x_test\n",
    "    x_pred1 = x_pred\n",
    "    p_test1 = scaler_y.inverse_transform(p_test)\n",
    "    p_pred1 = scaler_y.inverse_transform(p_pred)\n",
    "\n",
    "    # Save results if requested\n",
    "    if saveResults:\n",
    "        np.savetxt(ResultsFolder + '/x_test.csv',  x_test1, delimiter=\",\")\n",
    "        np.savetxt(ResultsFolder + '/x_pred.csv',  x_pred1, delimiter=\",\")\n",
    "        np.savetxt(ResultsFolder + '/p_test.csv',  p_test1, delimiter=\",\")\n",
    "        np.savetxt(ResultsFolder + '/p_pred.csv',  p_pred1, delimiter=\",\")\n",
    "\n",
    "    return x_test_mse, p_test_mse, test_kld_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, KLweight):\n",
    "    # Set property model to training mode\n",
    "    p_model.train()\n",
    "\n",
    "    # Initialize loss trackers\n",
    "    x_loss_mse = 0.0\n",
    "    p_loss_mse = 0.0\n",
    "\n",
    "    data    = train_prefetcher.next()\n",
    "    n_batch = 0\n",
    "\n",
    "    while data is not None:\n",
    "        n_batch += 1\n",
    "        if n_batch >= num_iters:\n",
    "            break\n",
    "\n",
    "        X = data[0].to(device)\n",
    "        Y = data[1].to(device)\n",
    "\n",
    "        data = train_prefetcher.next()\n",
    "\n",
    "        # Set training or eval mode based on model loading condition\n",
    "        if not load_pretrained_model:\n",
    "            model.train()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            model.eval()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Encode input\n",
    "        encoded, mu, std = model.encoder(X)\n",
    "        p_input = encoded if add_noise else mu\n",
    "\n",
    "        # Predict and decode\n",
    "        p_pred    = p_model(p_input)\n",
    "        x_decoded = model.decoder(encoded)\n",
    "\n",
    "        # Compute losses\n",
    "        x_train_mse = recon_criterion(x_decoded, X)\n",
    "        train_kld   = kld_loss(mu, std)\n",
    "        p_train_mse = properties_weighted_loss(p_pred, Y)\n",
    "\n",
    "        # Final loss depending on pretrained flag\n",
    "        if not load_pretrained_model:\n",
    "            loss = train_kld + x_train_mse * x_weight + p_train_mse * p_weight[epoch]\n",
    "        else:\n",
    "            loss = p_train_mse\n",
    "\n",
    "        # Backpropagation and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track batch losses\n",
    "        x_loss_mse += x_train_mse.item()\n",
    "        p_loss_mse += p_train_mse.item()\n",
    "\n",
    "    return x_loss_mse, p_loss_mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "XX = torch.tensor(XX, dtype=torch.float32)\n",
    "YY = torch.tensor(YY, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable anomaly detection for autograd debugging\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Initialize model weights\n",
    "model.apply(weights_init)\n",
    "p_model.apply(weights_init)\n",
    "\n",
    "# Set model to evaluation mode before training loop starts\n",
    "model.eval()\n",
    "saveResults = None\n",
    "\n",
    "# Prepare dataset and split into training and testing sets\n",
    "dataset = TensorDataset(XX, YY)\n",
    "num_train = len(dataset)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [num_train - split, split])\n",
    "\n",
    "# Create data loaders with pin_memory for better performance\n",
    "train_loader = DataLoaderX(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader  = DataLoaderX(test_dataset, batch_size=test_batch_size, shuffle=True, pin_memory=True)\n",
    "num_iters    = len(train_loader)\n",
    "\n",
    "# Set optimizer depending on whether pre-trained model is used\n",
    "if load_pretrained_model:\n",
    "    optimizer = torch.optim.Adam(p_model.parameters(), lr=c_lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(p_model.parameters()), lr=learningRate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Initialize tracking variables for best accuracy\n",
    "best_p_accuracy      = 0.0\n",
    "best_accuracy_file   = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of training epochs\n",
    "epochs = 200\n",
    "\n",
    "# Initialize loss tracking matrix if not resuming from best model\n",
    "if best_accuracy_file is None:\n",
    "    epoch_loss = [[0 for _ in range(6)] for _ in range(epochs)]\n",
    "\n",
    "    for epoch in trange(epochs):\n",
    "\n",
    "        # Start timer\n",
    "        start1 = time.time()\n",
    "\n",
    "        # Prepare data prefetcher and KL divergence weight\n",
    "        train_prefetcher = data_prefetcher(train_loader)\n",
    "        KLweight = 1.0\n",
    "\n",
    "        # Training step\n",
    "        x_loss_mse, p_loss_mse = train(epoch, KLweight)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Enable result saving in final epoch\n",
    "        if epoch == epochs - 1:\n",
    "            saveResults = True\n",
    "\n",
    "        # Evaluation step\n",
    "        x_test_mse, p_test_mse, test_kld = test(model, p_model, test_loader, saveResults, test_batch_size)\n",
    "\n",
    "        # Log metrics\n",
    "        epoch_loss[epoch][0] = epoch\n",
    "        epoch_loss[epoch][1] = x_test_mse\n",
    "        epoch_loss[epoch][2] = p_test_mse\n",
    "        epoch_loss[epoch][3] = x_loss_mse\n",
    "        epoch_loss[epoch][4] = p_loss_mse\n",
    "        epoch_loss[epoch][5] = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Print progress every 2 epochs or at last epoch\n",
    "        if (epoch % 2 == 0) or (epoch == epochs - 1):\n",
    "            print(\"Epoch:\", '%03d' % (epoch + 1), '/', str(epochs),\n",
    "                  \", x train =\", \"{:.5f}\".format(x_loss_mse / len(train_loader) / batch_size),\n",
    "                  \", x test =\", \"{:.5f}\".format(x_test_mse / len(test_loader) / test_batch_size),\n",
    "                  \", p train =\", \"{:.6f}\".format(p_loss_mse / len(train_loader) / batch_size),\n",
    "                  \", p test =\", \"{:.6f}\".format(p_test_mse / len(test_loader) / test_batch_size),\n",
    "                  \", KL weight =\", \"{:.5f}\".format(KLweight),\n",
    "                  \", time =\", \"{:.2f}\".format(time.time() - start1))\n",
    "\n",
    "            # Save best model based on property prediction loss\n",
    "            current_p_accuracy = p_test_mse / len(test_loader) / test_batch_size\n",
    "            if best_p_accuracy == 0.0 or current_p_accuracy < best_p_accuracy:\n",
    "                print('updating best p accuracy: previous best = {:.6f} new best = {:.6f}'.format(\n",
    "                    best_p_accuracy, current_p_accuracy))\n",
    "                best_p_accuracy = current_p_accuracy\n",
    "                torch.save(p_model.state_dict(), savedModelFolder + '/best_p_model.pt')\n",
    "                torch.save(model.state_dict(), savedModelFolder + '/best_model.pt')\n",
    "\n",
    "# Final model saving after training is done\n",
    "model.cpu()\n",
    "p_model.cpu()\n",
    "model.eval()\n",
    "p_model.eval()\n",
    "\n",
    "torch.save(model.state_dict(), savedModelFolder + '/model.pt')\n",
    "torch.save(p_model.state_dict(), savedModelFolder + '/p_model.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
